import argparse
import os
import shutil
import time
from pathlib import Path

import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random
import numpy as np
import pyrealsense2 as rs

from models.experimental import attempt_load
from utils.general import (
    check_img_size, non_max_suppression, apply_classifier, scale_coords,
    xyxy2xywh, plot_one_box, strip_optimizer, set_logging)
from utils.torch_utils import select_device, load_classifier, time_synchronized
from utils.datasets import letterbox

class YOLOv5RealSenseDetector:
    def __init__(self, opt):
        self.opt = opt
        self.device = select_device(opt.device)
        self.half = self.device.type != 'cpu'  # Half precision only supported on CUDA
        self.model = self.load_model(opt.weights)
        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
        self.colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(self.names))]
        self.pipeline = self.init_realsense()

    def load_model(self, weights):
        # Load model
        model = attempt_load(weights, map_location=self.device)  # load FP32 model
        self.opt.img_size = check_img_size(self.opt.img_size, s=model.stride.max())  # check img_size
        if self.half:
            model.half()  # to FP16
        return model

    def init_realsense(self):
        pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        try:
            pipeline.start(config)
        except RuntimeError as e:
            print(f"Error starting the pipeline: {e}")
        return pipeline

    def detect(self):
        view_img = True
        align_to_color = rs.align(rs.stream.color)

        while True:
            frames = self.pipeline.wait_for_frames()
            frames = align_to_color.process(frames)
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            if not depth_frame:
                print("Depth frame is empty. Cannot convert to numpy array.")
                continue  # Skip this frame if the depth frame is empty
            depth_image = np.asanyarray(depth_frame.get_data())
            color_image = np.asanyarray(color_frame.get_data())

            img = self.prepare_image(color_image)
            pred = self.run_inference(img)
            self.process_detections(pred, color_image, depth_frame)

            if view_img:
                cv2.imshow('Detection', color_image)
                if cv2.waitKey(1) == ord('q'):
                    break

    def prepare_image(self, color_image):
        img = [letterbox(color_image, new_shape=self.opt.img_size)[0]]
        img = np.stack(img, 0)
        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)
        img = np.ascontiguousarray(img, dtype=np.float16 if self.half else np.float32)
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        return torch.from_numpy(img).to(self.device)

    def run_inference(self, img):
        if img.ndimension() == 3:
            img = img.unsqueeze(0)
        with torch.no_grad():
            pred = self.model(img, augment=self.opt.augment)[0]
        return non_max_suppression(pred, self.opt.conf_thres, self.opt.iou_thres, classes=self.opt.classes, agnostic=self.opt.agnostic_nms)

    def process_detections(self, pred, color_image, depth_frame):
        for i, det in enumerate(pred):  # detections per image
            if det is not None and len(det):
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], color_image.shape).round()
                for *xyxy, conf, cls in reversed(det):
                    self.draw_bounding_box(xyxy, color_image, depth_frame, cls)

    def draw_bounding_box(self, xyxy, color_image, depth_frame, cls):
        mid_pos = [int((int(xyxy[0]) + int(xyxy[2])) / 2), int((int(xyxy[1]) + int(xyxy[3])) / 2)]
        distance_list = [depth_frame.get_distance(int(mid_pos[0]), int(mid_pos[1])) for _ in range(40)]
        distance_list = np.array(distance_list)
        distance_list = np.sort(distance_list)[10:30]
        label = f'{self.names[int(cls)]} {np.mean(distance_list):.2f}m'
        plot_one_box(xyxy, color_image, label=label, color=self.colors[int(cls)], line_thickness=3)

def main(opt):
    detector = YOLOv5RealSenseDetector(opt)
    detector.detect()

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='best.pt', help='model.pt path(s)')
    parser.add_argument('--source', type=str, default='inference/images', help='source')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='display results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-dir', type=str, default='inference/output', help='directory to save results')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    opt = parser.parse_args()

    main(opt)
